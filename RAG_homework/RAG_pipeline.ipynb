{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "сделаем RAG pipeline на основе конспектов по курсу машинки\n",
    "\n",
    "вот сурс:\n",
    "https://www.overleaf.com/read/gqpbbyphsjwt#4773bd\n",
    "\n",
    "об обработке данных:\n",
    "код находится в `RAG/data_parser`\n",
    "\n",
    "1) Так как у меня конспект, то каждой главе текста соответствует еще и название. Поэтому каждому куску текста кроме своего эмбеддинга сопоставлялся эмбеддинг названия главы. \n",
    "2) Эксперименты показали, что на моих данных лучше всего удалить всякие латеховские конструкции и большие формулы. А также поочистить криво поставленные знаки препинания. Это делается в соответствующей функции.\n",
    "3) По некоторым билетам отсутствуют текстовые ответы, поэтому модель не умеет отвечать на эти вопросы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os\n",
    "\n",
    "import torch\n",
    "\n",
    "from RAG.vectorizer import DBVectirizer\n",
    "from RAG.data_parser import parse_data\n",
    "from RAG.index_creator import load_index\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Парсим данные из директории с LaTeX кодом и сохраняем их в виде csv файла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем retriever. Код его составления вынесен в отдельный файл. в процессе работы не сохраняю получившуюся модель, поскольку данных немного.\n",
    "\n",
    "`model_name` указывает, какая модель используется для векторизации текста.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"t-tech/T-lite-it-1.0\"\n",
    "model_name = \"ai-forever/sbert_large_nlu_ru\"\n",
    "\n",
    "retriever = load_index(model_name = model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка модели. Используем VLLM  фреймворк, чтобы все быстро работало"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-19 10:44:36 config.py:510] This model supports multiple tasks: {'embed', 'reward', 'generate', 'classify', 'score'}. Defaulting to 'generate'.\n",
      "INFO 01-19 10:44:36 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='t-tech/T-lite-it-1.0', speculative_config=None, tokenizer='t-tech/T-lite-it-1.0', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=t-tech/T-lite-it-1.0, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 01-19 10:44:38 selector.py:120] Using Flash Attention backend.\n",
      "INFO 01-19 10:44:38 model_runner.py:1094] Starting to load model t-tech/T-lite-it-1.0...\n",
      "INFO 01-19 10:44:39 weight_utils.py:251] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a13fad00ce6e46e79901edf366c3e57b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-19 10:44:42 model_runner.py:1099] Loading model weights took 14.2426 GB\n",
      "INFO 01-19 10:44:44 worker.py:241] Memory profiling takes 2.09 seconds\n",
      "INFO 01-19 10:44:44 worker.py:241] the current vLLM instance can use total_gpu_memory (79.25GiB) x gpu_memory_utilization (0.90) = 71.33GiB\n",
      "INFO 01-19 10:44:44 worker.py:241] model weights take 14.24GiB; non_torch_memory takes 0.14GiB; PyTorch activation peak memory takes 4.35GiB; the rest of the memory reserved for KV Cache is 52.59GiB.\n",
      "INFO 01-19 10:44:44 gpu_executor.py:76] # GPU blocks: 61549, # CPU blocks: 4681\n",
      "INFO 01-19 10:44:44 gpu_executor.py:80] Maximum concurrency for 32768 tokens per request: 30.05x\n",
      "INFO 01-19 10:44:47 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:12<00:00,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-19 10:44:59 model_runner.py:1535] Graph capturing finished in 12 secs, took 0.22 GiB\n",
      "INFO 01-19 10:44:59 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 17.17 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import VLLM\n",
    "# model_name = \"ai-forever/sbert_large_nlu_ru\"\n",
    "# model_name = \"ai-forever/rugpt3large_based_on_gpt2\"\n",
    "# model_name = \"yahma/llama-7b-hf\"\n",
    "model_name = \"t-tech/T-lite-it-1.0\"\n",
    "\n",
    "llm_model = VLLM(\n",
    "    model=model_name,\n",
    "    device_map=\"cuda:1\",\n",
    "    max_new_tokens=500,\n",
    "    enforce_eager=True,\n",
    "    dtype=\"bfloat16\",\n",
    "    gpu_memory_utilization=0.8,\n",
    "    GPU_max_memory= {0: '0GiB', 1: '20GiB'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Делаем цепочку для обработки запроса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:24: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<>:24: SyntaxWarning: invalid escape sequence '\\c'\n",
      "/tmp/ipykernel_201890/674524840.py:24: SyntaxWarning: invalid escape sequence '\\c'\n",
      "  prompt = ChatPromptTemplate.from_template(\"\"\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Вы являетесь помощником при выполнении заданий по поиску ответов на вопросы.\n",
    "Используйте приведенные ниже фрагменты извлеченного контекста и только их, чтобы ответить на вопрос.\n",
    "Если в приведенном контексте нет ответа на вопрос, сообщите об этом.\n",
    "Ответ должен быть на русском языке.\n",
    "\n",
    "<context>\n",
    "Контекст: {context}\n",
    "<\\context>\n",
    "\n",
    "Вопрос: {input}\n",
    "Ответ:\n",
    "\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm_model, prompt)\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на несколько примеров ответов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.72s/it, est. speed input: 340.49 toks/s, output: 83.64 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Задача классификации заключается в том, чтобы разделять объекты на различные классы на основе их признаков. В контексте линейной классификации это означает нахождение гиперплоскости, которая максимально разделяет объекты разных классов. Основная цель — минимизировать ошибку классификации и обеспечить, чтобы между классами была достаточно широкая зона разделения, называемая зазором (margin). В конкретном контексте, описанном в тексте, задача классификации включает в себя:\n",
      "\n",
      "1. Нахождение вектора весов \\( w \\) и смещения \\( w_0 \\) таких, чтобы гиперплоскость \\( \\langle w, x_i \\rangle - w_0 \\) максимально разделяла классы, удовлетворяя условию \\( y_i(\\langle w, x_i \\rangle - w_0) > 0 \\) для всех \\( i \\).\n",
      "2. Максимизация зазора между гиперплоскостью и ближайшими объектами из каждого класса для улучшения качества разделения.\n",
      "3. Использование стратегий, таких как One vs One, для решения сложных случаев, когда невозможно разделить все классы одной гиперплоскостью.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"в чем заключается задача классификации?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.83s/it, est. speed input: 158.73 toks/s, output: 85.80 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Идея бустинга заключается в построении последовательности слабых классификаторов, которые последовательно \"усиливаются\" путем корректировки весов объектов в обучающей выборке. Каждый новый классификатор в этой последовательности обучается на модифицированном наборе данных, на котором предыдущие классификаторы ошиблись. Таким образом, создается более мощный композитный классификатор, который, в результате комбинирования слабых классификаторов, способен более точно отделять классы в данных, особенно в тех случаях, когда классы нелинейно разделимы или имеют сложные границы. В данном контексте, точная информация о бустинге отсутствует, так как он не упоминается в приведенном фрагменте текста. Контекст касается линейных классификаторов и их проблем при решении задач классификации, однако идея бустинга, которая связана с построением ансамблей слабых классификаторов, в этом фрагменте не рассматривается. \n",
      "\n",
      "Для ответа на вопрос предлагается обратиться к другим контекстам или литературе по теме бустинга. Если интересует более подробное объяснение, пожалуйста, предоставьте дополнительный контекст, содержащий информацию о бустинге. \n",
      "\n",
      "Однако, если исходить из контекста, связанного с линейными классификаторами и их проблемами, здесь обсуждаются методы, такие как стратегия \"One vs One\", где каждый класс сравнивается с каждым другим, но это не бустинг.\n",
      "\n",
      "Ответ на вопрос в контексте данного фрагмента: **Ответ не найден в данном контексте**.\n",
      "\n",
      "Если вы хотите получить более подробное объяснение идеи бустинга, вот краткое объяснение:\n",
      "\n",
      "Бустинг — это метод ансамблевого обучения, который состоит в обучении последовательности слабых классификаторов, где каждый следующий классиф\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"в чем состоит идея бустинга?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.88s/it, est. speed input: 213.99 toks/s, output: 84.98 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метод SVM (Support Vector Machine) применяется для решения задач классификации и регрессии. Он находит гиперплоскость (или гиперповерхность в многомерном пространстве), которая максимально разделяет объекты разных классов. В задачах классификации SVM стремится создать границу, которая с наименьшей вероятностью будет содержать объекты из разных классов, тем самым минимизируя обобщающую ошибку. В задачах регрессии SVM может использоваться для построения границы, которая наилучшим образом аппроксимирует зависимость между входными и выходными данными.\n",
      "\n",
      "Однако в данном контексте нет конкретной информации о том, как применяется метод SVM, поэтому давайте выясним это поэтапно на основе контекста:\n",
      "\n",
      "1. **Тренировочная и валидационная выборки**: Метод SVM часто сочетается с методами разделения данных на тренировочную и валидационную выборки. Например, использование train-test split или кросс-валидации помогает определить оптимальные гиперпараметры, такие как размер ядра или уровень регуляризации.\n",
      "\n",
      "2. **Гиперпараметры**: Подбор гиперпараметров SVM может быть критичным для его производительности. Метод кросс-валидации позволяет подобрать эти параметры так, чтобы модель достигла наилучшей производительности на валидационной выборке.\n",
      "\n",
      "3. **Проблема смещения и дисперсии**: SVM стремится минимизировать как смещение, так и дисперсию. Это позволяет ему избежать как переобучения, так и недообучения, находясь в оптимальной зоне между этими двумя крайностями.\n",
      "\n",
      "Таким образом, хотя в данном контексте нет прямого упоминания о SVM, из контекста можно понять, что методы, такие как кросс-валидация и работа с гиперпараметрами, важны для оптимизации производительности моделей, включая SVM. Однако, если речь конкретно идет\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"для чего применяется метод SVM?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## промежуточные выводы\n",
    "модель генерации текста работает хорошо, однако модель векторизации текста подтупливает и не находит нужных фрагментов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Постановка задач обучения с учителем (supervised learning).',\n",
       "  ' Постановка задачи с <<учителем>> подразумевает наличие целевых меток для предсказания (targets). Определим следующее:  Training set , где    для регрессии   для бинарной классификации   для многоклассовой классификации   Модель , которая предсказывает какое-то значение для каждого объекта.  Функцию потерь , которую мы будем минимизировать.  Формальнее, пусть имеется семейство моделей . Допустим, что оно параметризовано (можно и без параметризации) вектором , тогда задача оптимизации: найти , что . Как пример, линейные регрессионные модели параметризуются векторами весов,  (для bias term). Примеры задач обучения с учителем:   Задача классификации~--- задача обучения с учителем. У нас есть набор классов, для некоторого множества объектов есть ответы (знаем к какому классу они принадлежат), для некоторого другого множества нужно предсказать класс. Пример: предсказание вернет клиент банка кредит или нет по историческим данным.  Регрессия~--- задача обучения с учителем, в которой есть выборка объектов, с известным значением вещественной целевой функции и выборка объектов, для которых это целевое значение нужно предсказать. Предполагаем, что целевая функция - функция признаков объекта и некоторого небольшого шума (желательно белого). В нашем курсе рассматривалась в основном линейная регрессия - регрессия, в которой предполагается, что эта зависимость линейная. Пример использования: классификация текстов - признаки набор nграмм. '],\n",
       " ['Задачи обучения без учителя. Назвать хотя бы две.',\n",
       "  ' Задача обучения без учителя: у нас нет таргета. Примеры таких задач:   Задача кластеризации~--- задача обучения без учителя. Есть множество объектов нужно разбить их на группы так, чтобы “похожие” объекты оказались в одной, а непохожие в разных. Пример: есть разнородное множество объектов, для которых нужно решать какую-нибудь задачу, и хочется разбить его на кластера, чтобы в дальнейшем работать с ними по отдельности. Если еще конкретней, то можно рассмотреть рекомендацию товаров в магазине одежды. Ясно, что парням и девушкам нужно показывать разные рекомендации, поэтому множество потенциальных покупателей было бы неплохо разбить на кластеры.  Уменьшение размерности~--- задача обучения без учителя, в которой хочется построить отображение из многомерного пространства в пространство существенно меньшей размерности с минимальными “потерями”. Хотим либо уметь хорошо восстанавливать объекты обратно в многомерное пространство, либо чтобы в новом пространстве “похожие” объекты оказались близко, а “непохожие” далеко. Пример: есть большое количество признаков, многие из которых избыточны (например, они могут быть линейнозависимы), и мы хотим избавиться от лишних признаков. ']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from RAG.data_parser import preprocess_latex, parse_teormin_text\n",
    "\n",
    "filename = \"5_sem_ml/section/polidobro/teormin.tex\"\n",
    "qa_teormin = parse_teormin_text(filename)\n",
    "\n",
    "qa_teormin[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.90s/it, est. speed input: 227.12 toks/s, output: 84.75 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.32s/it, est. speed input: 589.85 toks/s, output: 81.80 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.11s/it, est. speed input: 436.89 toks/s, output: 83.00 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.39s/it, est. speed input: 568.23 toks/s, output: 83.27 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.79s/it, est. speed input: 406.59 toks/s, output: 82.90 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.85s/it, est. speed input: 142.87 toks/s, output: 85.45 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.46s/it, est. speed input: 326.69 toks/s, output: 84.02 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.84s/it, est. speed input: 178.89 toks/s, output: 85.67 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.86s/it, est. speed input: 215.01 toks/s, output: 85.32 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.03s/it, est. speed input: 481.68 toks/s, output: 83.41 toks/s]\n"
     ]
    }
   ],
   "source": [
    "model_answers = []\n",
    "for qa in qa_teormin[:10]:\n",
    "    question, answer = qa\n",
    "    response = retrieval_chain.invoke({\"input\": f\"{question}\"})\n",
    "    model_answer = response[\"answer\"]\n",
    "    line = [question, answer, model_answer]\n",
    "    model_answers.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Постановка задач обучения с учителем (supervised learning).',\n",
       "  ' Постановка задачи с <<учителем>> подразумевает наличие целевых меток для предсказания (targets). Определим следующее:  Training set , где    для регрессии   для бинарной классификации   для многоклассовой классификации   Модель , которая предсказывает какое-то значение для каждого объекта.  Функцию потерь , которую мы будем минимизировать.  Формальнее, пусть имеется семейство моделей . Допустим, что оно параметризовано (можно и без параметризации) вектором , тогда задача оптимизации: найти , что . Как пример, линейные регрессионные модели параметризуются векторами весов,  (для bias term). Примеры задач обучения с учителем:   Задача классификации~--- задача обучения с учителем. У нас есть набор классов, для некоторого множества объектов есть ответы (знаем к какому классу они принадлежат), для некоторого другого множества нужно предсказать класс. Пример: предсказание вернет клиент банка кредит или нет по историческим данным.  Регрессия~--- задача обучения с учителем, в которой есть выборка объектов, с известным значением вещественной целевой функции и выборка объектов, для которых это целевое значение нужно предсказать. Предполагаем, что целевая функция - функция признаков объекта и некоторого небольшого шума (желательно белого). В нашем курсе рассматривалась в основном линейная регрессия - регрессия, в которой предполагается, что эта зависимость линейная. Пример использования: классификация текстов - признаки набор nграмм. ',\n",
       "  'Обучение с учителем (supervised learning) включает в себя задачи, в которых имеется таргет, или целевая переменная, и цель состоит в том, чтобы научить модель предсказывать эту переменную на основе входных данных. В контексте текста, приведенном выше, задачи, которые обсуждаются, включают:\\n\\n1. **Линейная классификация**: Упомянутый функционал ошибки, который минимизирует долю неправильных ответов, является примером постановки задачи обучения с учителем. Цель здесь — обучить модель, которая правильно классифицирует объекты на основе их признаков.\\n\\n2. **Кластеризация**: Хотя это задача обучения без учителя, она упоминается как пример разбивки множества объектов на группы, где таргет отсутствует, но можно рассматривать задачу как попытку найти скрытые структуры в данных, которые могут быть полезны для дальнейшего использования (например, для задачи классификации).\\n\\n3. **Уменьшение размерности**: Это также задача обучения без учителя, но конкретно задача, в которой цель состоит в уменьшении количества признаков при сохранении основной информации. Если включить уменьшение размерности с целью последующей классификации или кластеризации, то это можно рассматривать как частичную задачу с учителем, однако в основном контексте текста она упоминается как задача без учителя.\\n\\nДля постановки задачи обучения с учителем в контексте линейной классификации, ключевым является наличие выходной переменной (таргета) $y_i$, и задача заключается в обучении линейного классификатора $a(x)$, который минимизирует долю неправильных ответов на основе признаков $x_i$ и таргета $y_i$.\\n\\nТаким образом, в данном контексте задача линейной классификации явно относится к постановкам'],\n",
       " ['Задачи обучения без учителя. Назвать хотя бы две.',\n",
       "  ' Задача обучения без учителя: у нас нет таргета. Примеры таких задач:   Задача кластеризации~--- задача обучения без учителя. Есть множество объектов нужно разбить их на группы так, чтобы “похожие” объекты оказались в одной, а непохожие в разных. Пример: есть разнородное множество объектов, для которых нужно решать какую-нибудь задачу, и хочется разбить его на кластера, чтобы в дальнейшем работать с ними по отдельности. Если еще конкретней, то можно рассмотреть рекомендацию товаров в магазине одежды. Ясно, что парням и девушкам нужно показывать разные рекомендации, поэтому множество потенциальных покупателей было бы неплохо разбить на кластеры.  Уменьшение размерности~--- задача обучения без учителя, в которой хочется построить отображение из многомерного пространства в пространство существенно меньшей размерности с минимальными “потерями”. Хотим либо уметь хорошо восстанавливать объекты обратно в многомерное пространство, либо чтобы в новом пространстве “похожие” объекты оказались близко, а “непохожие” далеко. Пример: есть большое количество признаков, многие из которых избыточны (например, они могут быть линейнозависимы), и мы хотим избавиться от лишних признаков. ',\n",
       "  'Задачи обучения без учителя можно назвать:\\n1. Задача кластеризации: включает в себя разбиение множества объектов на группы на основе схожести между ними, без наличия заранее известных меток. Примером может служить кластеризация покупателей магазина одежды для рекомендаций товаров.\\n2. Уменьшение размерности: задача, где необходимо отобразить объекты из многомерного пространства в пространство меньшей размерности, сохраняя при этом их схожесть или расстояние между ними. Это помогает в упрощении данных и уменьшении избыточности. Примером может быть уменьшение числа признаков в данных для улучшения интерпретируемости модели.']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_answers[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отобразим в виде таблицы. \n",
    "\n",
    "Видим, что модель на все отвечает хорошо. Однако местами нужный контекст не находится. Возможно это связано со способом построения эмбеддингов для текстов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border:1px solid black\" >\n",
       "  <tr>\n",
       "    <th style=\"text-align: center; border:1px solid black\">PROMPT</th>\n",
       "    <th style=\"text-align: center; border:1px solid black\">Answer</th>\n",
       "    <th style=\"text-align: center; border:1px solid black\">Model answer</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"width:15%; border:1px solid black; padding: 5px; white-space: pre-wrap;\">Постановка задач обучения с учителем (supervised learning).</td>\n",
       "    <td style=\"width:35%; border:1px solid black; padding: 5px; white-space: pre-wrap;\"> Постановка задачи с <<учителем>> подразумевает наличие целевых меток для предсказания (targets). Определим следующее:  Training set , где    для регрессии   для бинарной классификации   для многоклассовой классификации   Модель , которая предсказывает какое-то значение для каждого объекта.  Функцию</td>\n",
       "    <td style=\"width:40%; border:1px solid black; padding: 5px; white-space: pre-wrap;\">Обучение с учителем (supervised learning) включает в себя задачи, в которых имеется таргет, или целевая переменная, и цель состоит в том, чтобы научить модель предсказывать эту переменную на основе входных данных. В контексте текста, приведенном выше, задачи, которые обсуждаются, включают:\n",
       "\n",
       "1. **Лин</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"width:15%; border:1px solid black; padding: 5px; white-space: pre-wrap;\">Задачи обучения без учителя. Назвать хотя бы две.</td>\n",
       "    <td style=\"width:35%; border:1px solid black; padding: 5px; white-space: pre-wrap;\"> Задача обучения без учителя: у нас нет таргета. Примеры таких задач:   Задача кластеризации~--- задача обучения без учителя. Есть множество объектов нужно разбить их на группы так, чтобы “похожие” объекты оказались в одной, а непохожие в разных. Пример: есть разнородное множество объектов, для кото</td>\n",
       "    <td style=\"width:40%; border:1px solid black; padding: 5px; white-space: pre-wrap;\">Задачи обучения без учителя можно назвать:\n",
       "1. Задача кластеризации: включает в себя разбиение множества объектов на группы на основе схожести между ними, без наличия заранее известных меток. Примером может служить кластеризация покупателей магазина одежды для рекомендаций товаров.\n",
       "2. Уменьшение разм</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"width:15%; border:1px solid black; padding: 5px; white-space: pre-wrap;\">Что означает свойство i.i.d.?</td>\n",
       "    <td style=\"width:35%; border:1px solid black; padding: 5px; white-space: pre-wrap;\"> i.i.d = независимые и одинаково распределенные  Объект~--- -мерный вектор, порожденный из некоторого распределения (то есть случайной величиной), тут подразумевается независимость объектов как независимость таких случайных векторов.  Одинаково распределенные: порождены одинаковым процессом (с одина</td>\n",
       "    <td style=\"width:40%; border:1px solid black; padding: 5px; white-space: pre-wrap;\">свойство i.i.d. означает \"независимые и одинаково распределенные\".\n",
       "\n",
       "Контекст не содержит данной информации. В предоставленном контексте обсуждаются темы, связанные с линейной регрессией, градиентным бустингом, регуляризацией, но нет упоминания о свойстве i.i.d. Чтобы ответить на вопрос, нужно обрати</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"width:15%; border:1px solid black; padding: 5px; white-space: pre-wrap;\">Основная идея наивного Байесовского классификатора. В чём его наивность?</td>\n",
       "    <td style=\"width:35%; border:1px solid black; padding: 5px; white-space: pre-wrap;\">Основная идея: мы хотим использовать теорему Байеса. Для этого нам надо чтобы признаки были независимы (а это обычно неправда, например, температура в Цельсиях и Фаренгейтах).</td>\n",
       "    <td style=\"width:40%; border:1px solid black; padding: 5px; white-space: pre-wrap;\">Основная идея наивного Байесовского классификатора заключается в предположении о независимости всех признаков. Это позволяет факторизовать совместную вероятность, как произведение условных вероятностей каждого признака:\n",
       "\n",
       "$$P(x_i|y_i = C_k) = \\prod\\limits_{n = 1}^p P(x_i^n|y_i = C_k)$$\n",
       "\n",
       "Наивность кла</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"width:15%; border:1px solid black; padding: 5px; white-space: pre-wrap;\">Запишите формулы для модели линейной регрессии и для среднеквадратичной ошибки.</td>\n",
       "    <td style=\"width:35%; border:1px solid black; padding: 5px; white-space: pre-wrap;\">Пусть у нас есть матрица объектов  и столбец таргетов  Тогда модель линейной регрессии записывается так: f_w(X) = Xw =   Y Среднеквадратичная ошибка записывается так: , где ~--- мощность выборки. Оптимальная оценка для просто квадратичной нормы ищется так:  .  Рассмотрим   ^2 = (X - Z)^T(X - Z) = X^</td>\n",
       "    <td style=\"width:40%; border:1px solid black; padding: 5px; white-space: pre-wrap;\">Для модели линейной регрессии формула выглядит следующим образом:\n",
       "\\[ \\hat{w} = (X^TX + \\lambda I)^{-1}X^TY \\]\n",
       "где:\n",
       "- \\(\\hat{w}\\) — вектор весов,\n",
       "- \\(X\\) — матрица признаков,\n",
       "- \\(Y\\) — вектор целевых значений,\n",
       "- \\(\\lambda\\) — гиперпараметр, регулирующий важность минимизации нормы вектора весов,\n",
       "- \\(I</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"width:15%; border:1px solid black; padding: 5px; white-space: pre-wrap;\">Запишите формулу для одного шага градиентного спуска. Как модифицировать градиентный спуск для очень большой выборки?</td>\n",
       "    <td style=\"width:35%; border:1px solid black; padding: 5px; white-space: pre-wrap;\"> Градиент~--- вектор, направление которого совпадает с направлением наибольшего возрастания величины  , значение которой меняется от одной точки пространства к другой (скалярного поля), а по величине (модулю) равный скорости роста этой величины в этом направлении.  ~--- направление наибольшего убыва</td>\n",
       "    <td style=\"width:40%; border:1px solid black; padding: 5px; white-space: pre-wrap;\">Данная задача связана с объяснением градиентного спуска в контексте линейной регрессии с регуляризацией, но не содержит прямого упоминания о градиентном спуске в представленном контексте. Текст касается регуляризации, особенно в контексте нахождения аналитического решения методом наименьших квадрато</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"width:15%; border:1px solid black; padding: 5px; white-space: pre-wrap;\">Что такое правдоподобие, метод максимального правдоподобия? Является ли правдоподобие вероятностью?</td>\n",
       "    <td style=\"width:35%; border:1px solid black; padding: 5px; white-space: pre-wrap;\"> Правдоподобие , где ~--- вектор параметров, ~--- матрица признаков, а ~--- вектор target. Таким образом, правдоподобие~--- это  получить такую матрицу признаки-цели при данном значении вектора параметров распределения объектов.  Метод максимального правдоподобия~--- метод, при котором правдоподобие</td>\n",
       "    <td style=\"width:40%; border:1px solid black; padding: 5px; white-space: pre-wrap;\">Правдоподобие в данном контексте описывается как условная вероятность выборки при условии параметрического семейства, которое её описывает, с формулой:\n",
       "\n",
       "$$L(\\theta | X, Y ) = P(X, Y |\\theta)$$\n",
       "\n",
       "Мы стремимся максимизировать это правдоподобие, чтобы найти наиболее вероятные параметры:\n",
       "\n",
       "$$L(\\theta | X,</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"width:15%; border:1px solid black; padding: 5px; white-space: pre-wrap;\">Что такое кросс-валидация? На что влияет количество блоков в кросс-валидации?</td>\n",
       "    <td style=\"width:35%; border:1px solid black; padding: 5px; white-space: pre-wrap;\">Для работы модели нужно определить ее гиперпараметры. Первая идея (не кросс-валидация): обучить модель по части train и по предсказаниям на второй части определить оптимальные гиперпараметры. Минус такого подхода в том, что модель будет зависить от случайного выбора куска трейна (а вдруг объекты в t</td>\n",
       "    <td style=\"width:40%; border:1px solid black; padding: 5px; white-space: pre-wrap;\">Кросс-валидация (CV) — это метод оценки производительности модели машинного обучения. Он позволяет оценить, насколько хорошо модель обобщает данные, а не просто запомнит их. Кросс-валидация делает это путем разделения данных на несколько \"блоков\" или \"фолдов\" и последовательного обучения модели на в</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"width:15%; border:1px solid black; padding: 5px; white-space: pre-wrap;\">Что такое переобучение и недообучение? Как их можно детектировать?</td>\n",
       "    <td style=\"width:35%; border:1px solid black; padding: 5px; white-space: pre-wrap;\"> Недообучение~--- ситуация, когда модель уловила не все общие закономерности и не способна достаточно точно воспроизвести распределение, из которого создаются объекты.  Переобучение~--- ситуация, когда модель не только успешно смоделировала распределение, но и включила в него шумовые факторы (то ест</td>\n",
       "    <td style=\"width:40%; border:1px solid black; padding: 5px; white-space: pre-wrap;\">Переобучение и недообучение - это феномены, которые могут возникать при обучении машинных моделей, особенно в контексте построения моделей для классификации. \n",
       "\n",
       "- **Переобучение** происходит, когда модель слишком сложна для обучающего набора данных и слишком точно подстраивается под шум или случайные</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"width:15%; border:1px solid black; padding: 5px; white-space: pre-wrap;\">Чем гиперпараметры отличаются от параметров? Что является параметрами и гиперпараметрами в линейных моделях и в решающих деревьях?</td>\n",
       "    <td style=\"width:35%; border:1px solid black; padding: 5px; white-space: pre-wrap;\">Параметры настраиваются непосредственно при обучении (например веса в линейной регерссии), в то время как гиперпараметры фиксированные и изменяются вручную, если мы понимаем, что модель учится плохо. Линейные модели:   Гиперпараметры:   Тип регуляризации (может быть структурным параметром)  Параметр</td>\n",
       "    <td style=\"width:40%; border:1px solid black; padding: 5px; white-space: pre-wrap;\">Гиперпараметры отличаются от параметров тем, что они не изменяются в процессе обучения модели, а задаются заранее.\n",
       "\n",
       "В линейных моделях гиперпараметрами являются:\n",
       "- Степень регуляризации (например, параметр $C$ в уравнении для функции потерь).\n",
       "\n",
       "Параметрами в линейных моделях являются:\n",
       "- Вектор весов </td>\n",
       "  </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This template helps to compare generated code samples in pretty table form\n",
    "# feel free to present your work in other forms\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "table_template = \"\"\"<table style=\"border:1px solid black\" >\n",
    "  <tr>\n",
    "    <th style=\"text-align: center; border:1px solid black\">PROMPT</th>\n",
    "    <th style=\"text-align: center; border:1px solid black\">Answer</th>\n",
    "    <th style=\"text-align: center; border:1px solid black\">Model answer</th>\n",
    "  </tr>\n",
    "{}\n",
    "</table>\"\"\"\n",
    "\n",
    "row_template = \"\"\"  <tr>\n",
    "    <td style=\"width:15%; border:1px solid black; padding: 5px; white-space: pre-wrap;\">{}</td>\n",
    "    <td style=\"width:35%; border:1px solid black; padding: 5px; white-space: pre-wrap;\">{}</td>\n",
    "    <td style=\"width:40%; border:1px solid black; padding: 5px; white-space: pre-wrap;\">{}</td>\n",
    "  </tr>\"\"\"\n",
    "\n",
    "rows = []\n",
    "\n",
    "show_first = 300\n",
    "for qas in model_answers:\n",
    "    question, answer, model_answer = qas\n",
    "    # replace placeholders in the format() arguments\n",
    "    rows.append(\n",
    "        row_template.format(\n",
    "            question, answer[:show_first].replace(\"<s>\", \"\"), model_answer[:show_first].replace(\"<s>\", \"\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "display(HTML(table_template.format(\"\\n\".join(rows))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
