Рассмотрим выборку $X$ размера $m$. Выберем $m$ объектов с повторениями из $X$ и получим \textit{бутстрапированную выборку}. Повторим данный процесс $N$ раз чтобы сгенерировать $N$ выборок $X_j$.

Мы сделали это для того, чтобы получить много выборок похожих на $X$, но при этом не совпадающих с ним. 

Возьмем нашу модель. Обучим ее на каждом $X_j$ поотдельности. Введем обозначения

\[ \varepsilon_j(x) = b_j(x)-y(x)\]

\[ \E (b_j(x) - y(x))^2=\E_x \varepsilon_j^2(x)\]

\[ \E_1 = \frac 1 N \sum_{j=1}^N \E_x \varepsilon_j^2(x),\]

где $b_j(x)$ - предсказание модели, обученной на выборке $X_j$, $y(x)$ - истинный ответ для объекта $x$, $\varepsilon_j(x)$ - ошибка $j$-ой модели, $\E_1$ - средняя ошибка всех моделей

Предположим, что ошибки моделей обученных на разных бутстрапированных выборках несмещены и нескоррелированы, т.е.

\[ \E_x \varepsilon_j(x) = 0\]

\[ \E_x \varepsilon_i(x) \varepsilon_j(x)=0, \quad i \neq j\]

Рассмотрим модель, которая усредняет предсказния всех моделей полученных ранее. Обозначим ее предсказание за $a(x)$. Тогда

\[ a(x)=\frac 1 N \sum_{j=1}^N b_j(x)\]

Найдем ошибку этой модели $\E_N$

\[ \E_N = \E_x \left( a(x) - y(x)\right)^2=\E_x \left( \frac 1 N \sum_{j=1}^N b_j(x) - y(x)\right)^2 = \E_x \left(\frac 1 N \sum_{j=1}^N \varepsilon_j(x) \right)^2= \] 

\[ = \frac{1}{N^2} \E_x \left( \sum_{j=1}^N \varepsilon_j^2(x) + \underbrace{\sum_{i \neq j} \varepsilon_i(x) \varepsilon_j(x)}_{=0} \right) = \frac 1 N E_1 \]

Получили, что ошибка упала в $N$ раз. Что мы сделали не так? (если все хорошо, то изобрели какой-то гениальный метод и вся остальная машинка не нужна)

Ответ: предположение о несмещенности и нескоррелированности не всегда верно на практике. 

Попробуем это исправить: с ошибкой каждой конкретной модели мы мало что можем сделать, но можем попробовать сделать так, чтобы ошибки разных моделей были не похожи друг на друга. Это должно уменьшить корреляцию ошибок и соответственно приблизить сумму из равенства выше к 0 и приблизиться к теоретическому результату. Воспользуемся слабостью деревьев: их склонностью к переобучению. Переобучим каждую модель под конкретную бутстрапную выборку, и потом усредним результаты. 

Но бутсрапные выборки все же очень похожи, поэтому это не приведет нас к желаемому результату. Поэтому мы в каждой выборке хотим выбрать признаки, на которых будет обучаться модель. Подробнее об этом поговорим в следующем билете

\begin{definition}
    Прием усреднения предсказаний модели, обученной на разных бутстрапных выборках называется \textit{бэггингом} (bagging = bootstrap aggregation)
\end{definition}

\begin{lemmanote}
    Усреднение по моделям работает только в том случае, если предсказания моделей осмысленны и мы просто хотим уменьшить дисперсию (понятно, что если модели выдают что-то рандомное никакое усреднение нам не поможет)
\end{lemmanote}