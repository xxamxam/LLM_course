\subsection{Постановка задач обучения с учителем (supervised learning).}

\Note Постановка задачи с <<учителем>> подразумевает наличие целевых меток для предсказания (targets).

Определим следующее:

\Def Training set $\mathcal{L} = \{ x_i, y_i \}_{i=1}^{n}$, где

\begin{itemize}
    \item $(x \in \R^p, y \in \R)$ для регрессии
    \item $x_i \in \R^p, y_i \in \{ 0, 1 \}$ для бинарной классификации
    \item $x_i \in \R^p, y_i \in \{ c_1, \ldots, c_n \}$ для многоклассовой классификации
\end{itemize}

\Def Модель $f(X)$, которая предсказывает какое-то значение для каждого объекта.

\Def Функцию потерь $Q(X,y,f)$, которую мы будем минимизировать.

\Note Формальнее, пусть имеется семейство моделей $\mathcal{F}$. Допустим, что оно параметризовано (можно и без параметризации) вектором $\theta \in \Theta$, тогда задача оптимизации: найти $f \in \mathcal{F}$, что $f = \arg\min\limits_{\theta} Q(X, y, f_\theta)$. Как пример, линейные регрессионные модели параметризуются векторами весов, $\Theta = \R^{p + 1}$ (для bias term). 

Примеры задач обучения с учителем:

\begin{itemize}
    \item Задача классификации~--- задача обучения с учителем. У нас есть набор классов, для некоторого множества объектов есть ответы (знаем к какому классу они принадлежат), для некоторого другого множества нужно предсказать класс. Пример: предсказание вернет клиент банка кредит или нет по историческим данным.
    \item Регрессия~--- задача обучения с учителем, в которой есть выборка объектов, с известным значением вещественной целевой функции и выборка объектов, для которых это целевое значение нужно предсказать. Предполагаем, что целевая функция - функция признаков объекта и некоторого небольшого шума (желательно белого). В нашем курсе рассматривалась в основном линейная регрессия - регрессия, в которой предполагается, что эта зависимость линейная. Пример использования: классификация текстов - признаки набор nграмм.
\end{itemize}


\subsection{Задачи обучения без учителя. Назвать хотя бы две.}

\Note Задача обучения без учителя: у нас нет таргета. 

Примеры таких задач:

\begin{itemize}
    \item Задача кластеризации~--- задача обучения без учителя. Есть множество объектов нужно разбить их на группы так, чтобы “похожие” объекты оказались в одной, а непохожие в разных. Пример: есть разнородное множество объектов, для которых нужно решать какую-нибудь задачу, и хочется разбить его на кластера, чтобы в дальнейшем работать с ними по отдельности. Если еще конкретней, то можно рассмотреть рекомендацию товаров в магазине одежды. Ясно, что парням и девушкам нужно показывать разные рекомендации, поэтому множество потенциальных покупателей было бы неплохо разбить на кластеры.
    
    \item Уменьшение размерности~--- задача обучения без учителя, в которой хочется построить отображение из многомерного пространства в пространство существенно меньшей размерности с минимальными “потерями”. Хотим либо уметь хорошо восстанавливать объекты обратно в многомерное пространство, либо чтобы в новом пространстве “похожие” объекты оказались близко, а “непохожие” далеко. Пример: есть большое количество признаков, многие из которых избыточны (например, они могут быть линейнозависимы), и мы хотим избавиться от лишних признаков.
\end{itemize}




\subsection{Что означает свойство i.i.d.?}

\Def i.i.d = независимые и одинаково распределенные

\Note Объект~--- $p$-мерный вектор, порожденный из некоторого распределения (то есть случайной величиной), тут подразумевается независимость объектов как независимость таких случайных векторов.

\Note Одинаково распределенные: порождены одинаковым процессом (с одинаковым распределением)

\textbf{Это условие мы требуем от объектов}





\subsection{Основная идея наивного Байесовского классификатора. В чём его наивность?}

Основная идея: мы хотим использовать теорему Байеса. Для этого нам надо чтобы признаки были независимы (а это обычно неправда, например, температура в Цельсиях и Фаренгейтах).





\subsection{Запишите формулы для модели линейной регрессии и для среднеквадратичной ошибки.}

Пусть у нас есть матрица объектов $X = (x_0, x_1, \ldots, x_n)^T$ и столбец таргетов $Y = (y_1, \ldots, y_n)^T$

Тогда модель линейной регрессии записывается так:

$$f_w(X) = Xw = \hat{Y} \approx Y$$

Среднеквадратичная ошибка записывается так: $Q_{MSE}(X) = \frac{1}{n}(Y - Xw)^T (Y-Xw)$, где $n$~--- мощность выборки. 

Оптимальная оценка для просто квадратичной нормы ищется так: 

\Lemma $\hat{\theta} = \brackets{Z^T Z}^{-1} Z^T X$.

\Proof Рассмотрим
\begin{equation*} \label{lreg_1}
    \norm{X - Z\theta}^2 = (X - Z\theta)^T(X - Z\theta) = X^TX - 2X^TZ\theta + \theta^TZ^TZ\theta
\end{equation*}
Заметим, что перед нами <<квадратный трехчлен>> по $\theta$. Продифференцируем по $\theta_i$:
\begin{equation*}
    -2(X^TZ)_i + 2(\theta^TZ^TZ)_i = 0 \Longrightarrow \theta^TZ^TZ - X^TZ = \vec{0} \Longrightarrow Z^TZ\theta = Z^TX \Longrightarrow \htheta = \brackets{Z^T Z}^{-1} Z^T X
\end{equation*}
\Endproof

\Statement $\htheta$ — несмещённая, то есть $\E\htheta = \theta$; и $\Var\htheta = \sigma^2\brackets{Z^T Z}^{-1}$.

\Th Теорема о наилучшей оценке в классе линейных оценок (б/д).

\Note Наилучшая или <<оптимальная>>~--- достигается равенство в неравенстве Рао-Крамера (или обладает минимальной дисперсией).

\Thbd Пусть $t = T\theta$, где $T \in \text{Mat}_{m \times k}(\R)$. Тогда ОНК имеет вид $\hat{t} = T\htheta$ и является оптимальной оценкой $t$ в классе линейных несмещённых оценок, то есть представимых в виде $B\cdot X$.





\subsection{Запишите формулу для одного шага градиентного спуска. Как модифицировать градиентный спуск для очень большой выборки?}

\Def Градиент~--- вектор, направление которого совпадает с направлением наибольшего возрастания величины $\varphi$ , значение которой меняется от одной точки пространства к другой (скалярного поля), а по величине (модулю) равный скорости роста этой величины в этом направлении.

\Note $\nabla f(x$~--- направление наибольшего убывания фукнции.

Введем следующие переменные:

\begin{itemize}
    \item $n$ ~--- число объектов в выборке
    \item $x_i$ ~--- вектор признаков $i$-го объекта
    \item $y_i$ ~--- таргет
    \item $w$ ~--- вектор весов (его ищем для поиска оптимальной модели)
    \item $f(w,x)$ ~--- функция предсказания целевой функции
    \item $L(y, \hat{y})$ ~--- функция потерь для одного объекта
\end{itemize}

Тогда формула для градиентного спуска выглядит так: ($\beta_t$ ~--- шаг градиентного спуска)
$$w_t = w_{t-1} - \beta_t\sum\limits_{i=1}^{n} \nabla_wL(y_i, f(w_{t-1}, x_i))$$

\Note Шаг градиентного спуска может выбираться на каждом шаге, а может быть константой

\Note Для большой выборки можно на каждом шаге суммировать не по всем объектам, а по некоторому небольшому случайному подмножеству элементов (SGD) (не нужно хранить в памяти значения вектора градиента по всей выборке).









\subsection{Что такое правдоподобие, метод максимального правдоподобия? Является ли правдоподобие вероятностью?}

\Def Правдоподобие $L(\theta \ | \ X, Y) = P(X, Y \ | \ \theta)$, где $\theta$~--- вектор параметров, $X$~--- матрица признаков, а $Y$~--- вектор target.

Таким образом, правдоподобие~--- это \textbf{вероятность} получить такую матрицу признаки-цели при данном значении вектора параметров распределения объектов.

\Def Метод максимального правдоподобия~--- метод, при котором правдоподобие стараются максимизировать (спасибо, кэп)






\subsection{Что такое кросс-валидация? На что влияет количество блоков в кросс-валидации?}

Для работы модели нужно определить ее гиперпараметры.

Первая идея (не кросс-валидация): обучить модель по части train и по предсказаниям на второй части определить оптимальные гиперпараметры. Минус такого подхода в том, что модель будет зависить от случайного выбора куска трейна (а вдруг объекты в train упорядочены по алфавиту и не все объекты попадут в обучающую выборку при определении гиперпараметров. Модель с этими гиперпараметрами может показать плохой результат на всей обучающей выборке).

Вторая идея (кросс-валидация): часть объектов из train идет на валидацию, часть на train, затем данный процесс повторяется несколько раз (часть с валидацией меняется). Тогда мы можем измерить дисперсию ошибки для каждого набора гиперпараметров и сам лосс, чтобы выбрать оптимальный набор.

\Note Далее речь идет о K-fold кросс-валидации, где $K$~--- число запусков кросс-валидации и одновременно то, на сколько примерно равных кусков мы бьем train. Тогда в рамках каждого запуска один из $K$ блоков является валидацией, а остальные~--- train.


Влияние числа блоков:

\begin{itemize}
    \item При увеличении числа блоков дисперсия ответа уменьшается (закономерно, ибо объем данных увеличивается)
    \item Аналогичная ситуация с bias - матожидание разности между истинным ответом и выданным алгоритмом - он уменьшается
    \item Чем меньше количество блоков, тем быстрее это работает, но тем меньше шанс, что модель подстроится под какой-то кусок данных.
\end{itemize}








\subsection{Что такое переобучение и недообучение? Как их можно детектировать?}

\Def Недообучение~--- ситуация, когда модель уловила не все общие закономерности и не способна достаточно точно воспроизвести распределение, из которого создаются объекты.

\Def Переобучение~--- ситуация, когда модель не только успешно смоделировала распределение, но и включила в него шумовые факторы (то есть переобучилась под выбросы).

Как детектировать это все?

Вспомним про train и test, будем измерять лосс на них в зависимости от сложности модели (например, число эпох или норма вектора весов в линейной регрессии).
\begin{itemize}
    \item Если и на train, и на test лосс падает, то мы уловили не все основополагающие зависимости $\Longrightarrow$ недообучение.
    \item Если на train лосс падает, а на test лосс растет, то мы уловили шумовые зависимости, чуждые распределению $\Longrightarrow$ переобучение.
\end{itemize}










\subsection{Чем гиперпараметры отличаются от параметров? Что является параметрами и гиперпараметрами в линейных моделях и в решающих деревьях?}

Параметры настраиваются непосредственно при обучении (например веса в линейной регерссии), в то время как гиперпараметры фиксированные и изменяются вручную, если мы понимаем, что модель учится плохо.

Линейные модели:

\begin{itemize}
    \item Гиперпараметры:
    \begin{enumerate}
        \item Тип регуляризации (может быть структурным параметром)
        \item Параметр регуляризации $\lambda$ (при использовании регуляризатора)
        \item Степень полинома в задаче регрессии с семейством алгоритмов, заданным множеством полиномов определенной степени.
    \end{enumerate}
    
    \item Параметры:
    
    \begin{enumerate}
        \item Матрица весов
        \item Вектор смещений
    \end{enumerate}
\end{itemize}

Решающие деревья:

\begin{itemize}
    \item Гиперпараметры:
    \begin{enumerate}
        \item Максимальная глубина
        \item Минимальное число элементов в листьях (и выбор условия в листе)
    \end{enumerate}
    
    \item Параметры:
    
    \begin{enumerate}
        \item Признаки (номер признака по которому производится разбиение $i$)
        \item Пороги (трешхолд t)
    \end{enumerate}
\end{itemize}











\subsection{Что такое регуляризация? Чем на практике отличается $L_1$-регуляризация от $L_2$?}

\Def Регуляризация~--- внесение в Loss члена, цель которого штрафовать за сложность модели. Например, для линейной регрессии такой член имеет вид $\lambda \cdot g(\omega)$, где $g$~--- монотонно возрастающая по каждому аргументу функция.

\Def $L_1$-регуляризация имеет вид $\lambda \cdot \sum\limits_{i = 1}^p |\omega_i| = \lambda\norm{\omega}_{L_1}$ ($\lambda > 0$)

\Def $L_2$-регуляризация имеет вид $\lambda \cdot \sqrt{\sum\limits_{i = 1}^p |\omega_i|^2} = \lambda\norm{\omega}_{L_2}$ ($\lambda > 0$)

Заметим, что $L_1$, что $L_2$ регуляризации в случае если $\lambda$ велико стремятся занулить вектор весов, так как иначе вклад члена регуляризации слишком большой. Но $L_1$ делает это значительно агрессивнее. Таким образом, $L_1$ стремится отобрать информативные признаки. 

Но при этом loss для $L_1$ регуляризации недифференцируем, в отличие от $L_2$, да еще и $L_2$ имеет аналитическое решение.








\subsection{Учитывается ли коэффициент сдвига $\omega$ в регуляризаторе? Почему?}

\textbf{Ответ:} нет.

\textbf{Объяснение:} допустим наша выборка является прямой $y = kx + 10^6$, тогда включение bias term в регуляризацию будет стремиться подавить его значение, что не позволит нам добрать нужное его значение. Более того, итоговая модель будет выглядеть как $Y = WX + b$, где $b$ как-то от $X$ не очень-то зависит, то есть $W$ отвечает за угадывание направления главного тренда, а $b$ за его смещение, которое является вторичным.








\subsection{Почему линейные модели рекомендуется применять к выборке с нормированными значениями признаков?}

Разберем два случай, есть ли регуляризация, или ее нет:
\begin{enumerate}
    \item Пусть есть регуляризация, тогда на ненормированной выборке можно получить проблему того, что некоторые веса будут огромными, так как масштабы признаков разные. Это приведет к взрыву нормы вектора весов и к стремлению его подавить (хотя этот признак с огромным коэффициентом мог быть самым информативным). Это большая проблема.
    \item Пусть нет регуляризации, тогда заметим, что нормализация является линейным отображением, а значит линейная можель может включить ее в себя. Но в любом случае масштаб данных разный, значит веса разные, а значит мы не можем проверить, какой признак для нас значим, так как чес больше по модулю вес, тем он значимее. 
    
    Как пример, можно поразмышлять над тем, как связаны веса и корреляции признака и target, в нормированных данных это даст почти прямую взаимосвязь, чем выше корреляция, тем выше значимость (информативность) признака (банально можно вспомнить PCA/SVD).
\end{enumerate}











\subsection{Запишите формулу для линейной модели классификации. Что такое отступ?}

Линейная модель классификации (предсказание класса для одного объекта х):

$$a(x) = \text{sign}(\langle w, x\rangle + w_0) = \text{sign}\left(\sum\limits_{j=1}^Nw_jx_j + w_0\right)$$

Далее, чтобы было проще,  добавим к матрице признаков единичный столбец, который будет отвечать за сдвиг $w_0$.

В случае бинарной классификации удобнее всего в качестве функционала ошибки использовать долю правильных ответов:

$$Q(a, X, y) = \dfrac{1}{l}\sum\limits_{i=1}^l[a(x_i) = y_i]\ (X - \text{матрица, состоящая из } x_i)$$

Нам удобнее решать задачу минимизации, поэтому будем вместо этого использовать долю неправильных ответов:

$$\begin{array}{rl}Q(a, X, y) &= \dfrac{1}{l}\sum\limits_{i=1}^l[a(x_i) \neq y_i] = \dfrac{1}{l}\sum\limits_{i=1}^l[\text{sign}(\langle w, x_i\rangle) \neq y_i] =\\&= \dfrac{1}{l}\sum\limits_{i=1}^l[y_i\langle w, x_i\rangle) < 0] \longrightarrow \min\limits_w.\end{array}$$

Величина в скобках называется отступом (margin). Отступ i-го объекта:

$$M_i = y_i\langle w, x_i\rangle$$

Знак отступа говорит о корректности ответа классификатора, а его абсолютная величина характеризует степень уверенности классификатора в своём ответе.















\subsection{Что такое точность и полнота? Почему нужно учитывать их вместе?}


\begin{center}
    \includegraphics[scale=1]{2_1.png}
\end{center}

\Def Точность (Precision) ~--- доля правильных предсказаний положительного класса

$$\textbf{Precision} = \frac{\textbf{True positive}}{\textbf{True positive + False positive}}$$

\Def Полнота (Recall) ~--- доля найденных положительных классов 

$$\textbf{Recall} = \frac{\textbf{True positive}}{\textbf{True positive + False negative}}$$

Эти два параметры тесно связаны между собой. Precision показывает насколько можно доверять классификатору в случае срабатывания, а Recall показывает на какой доле истинных объектов классификатор сработал.

Примеры:

\begin{itemize}
    \item Первый пример — использование в задаче кредитного скоринга. Пусть в задаче кредитного скоринга ставится условие, что неудачных кредитов должно быть не больше 5\%. В таком случае задача является задачей максимизации полноты при условии $Precision(a,X) \Ge 0.95$.
    \item Второй пример — использование в медицинской диагностике. Необходимо построить модель, которая определяет, есть или нет определенное заболевание у пациента. При этом требуется, чтобы были выявлены как минимум 80\% пациентов, которые действительно имеют данное заболевание. Тогда ставят задачу максимизации точности при условии $recall(a,X) \Ge 0.8$.
\end{itemize}











\subsection{В задаче бинарной классификации доля одного класса составляют 95\% выборки. Какие
метрики разумно использовать для оценки работы модели? Почему?}

Осознаем, что ROC-AUC не зависит от сбалансированности классов, так как строится в осях $FPR = \frac{FP}{FP + TN}$ и $TPR = \frac{TP}{TP + FN}$, то есть доли ответов нормируются на размеры классов, а значит процентное соотношение не задушит нас, тогда как задушит остальные метрики.






\subsection{Что такое ROC-AUC? Как построить ROC-кривую?}

\Def \textbf{ROC} ~--- Receiver Operating Characteristic

\Def $\textbf{TPR} = \frac{\textbf{True positive}}{\textbf{True positive + False negative}} = \textbf{Recall}$

\Def $\textbf{FPR} = \frac{\textbf{False positive}}{\textbf{False positive + True negative}}$

Отсортируем объекты по вероятности предсказания позитивного/негативного класса, положим их на одну ось. Будем идти скользящей границей (treshold) от минимальной вероятности к максимальной. Считаем 4 характеристики из нашей любимой таблички: FP, FN, TP, TN

TPR и FPR - это наши оси

Полученный график характеристик в данных осях называется ROC-кривой. Посмотрим на ее свойства:

\begin{itemize}
    \item Разумный классификатор всегда выше диагонали (диагональ это рандомный)
    \item Если кривая строго ниже диагонали меняем знак предсказания
    \item Если одна кривая строго выше другой, то соответствующий классификатор лучше
\end{itemize}

\Def \textbf{ROC - AUC} = ROC Area Under Curve - площадь под ROC-кривой: чем лучше классификатор, тем больше (обратное неверно)








\subsection{Запишите функционал логистической регрессии. Как он связан с методом максимума
правдоподобия?}

Попробуем предсказать вероятность объекта быть 1-м классом:

$$p_+ = P(y = 1 | x) \in [0, 1]$$

Но вероятность живет на отрезке, а нам нужно $\R$, тогда нам нужна функция, которая будет имитировать вероятность и иметь значения на $\R$

$$p_+ = \frac{1}{1 + \exp(-x^T\omega)} = \sigma(x^T\omega)$$

Функционал: $L_{\text{Logistic}} = \log(1 + \exp(-M_i))$

\Note $\sigma_{\omega}(x) = \sigma(x^T\omega)$

\Note $\log L(\omega | X, Y) = \log P (X, Y | \omega) = \log \prod \limits_{i=1}^{n} P(x_i, y_i | \omega)$

\Note сигмоида симметрична относительно $(0, 0.5)$

Тогда можем записать следующее:

\begin{itemize}
    \item if $y_i == 1 : P(x_i, 1 | \omega) = \sigma_{\omega} (x_i) = \sigma(M_i)$
    \item if $y_i == -1 : P(x_i, -1 | \omega) = 1 - \sigma_{\omega} (x_i) = \sigma_{\omega} (-x_i) = \sigma(M_i)$
\end{itemize}

Тогда получим искомую связь: $$\log L(\omega | X, Y) = \sum \limits_{i=1}^n \log \sigma(M_i) = 
- \sum \limits_{i=1}^n \log (1 + \exp(-M_i)) \to \max$$









\subsection{Идея метода опорных векторов (в случае разделимой выборки).}

Пусть выборка линейно разделима, то есть существует некоторая гиперплоскость, разделяющая классы -1 и +1. Тогда в качестве алгоритма классификации можно использовать линейный пороговый классификатор:

$$a(x) = \text{sign}(\langle w, x\rangle + w_0) = \text{sign}\left(\sum\limits_{j=1}^Nw_jx_j + w_0\right)$$

Но для двух линейно разделимых классов возможно много вариантов построения гиперплоскости. Метод опорных векторов выбирает ту гиперплоскость, которая максимизирует отступ между классами:

\Remind $M_i = y_i\langle w, x_i\rangle$ (считаем что $w_0$ "вшито" в произведение)

\Note Чем меньше значение отступа $M_i$ тем ближе объект к границе классов

\Def Метод опорных векторов (SVM)~--- один из наиболее популярных методов обучения, который применяется для решения задач классификации и регрессии. Основная идея метода заключается в построении гиперплоскости, разделяющей объекты выборки оптимальным способом. Алгоритм работает в предположении, что чем больше расстояние (зазор) между разделяющей гиперплоскостью и объектами разделяемых классов, тем меньше будет средняя ошибка классификатора.














\subsection{Опишите жадный алгоритм обучения решающего дерева.}

В ходе построения дерева мы разбиваем вашу выборку на две части. Для этого представим, что у нас есть $L$ и $R$~--- левое и правое поддерево объектов, тогда вспомним, что разбиение в узле строится по критерию вида $I[x_i^j < b]$, $b$~--- порог, $x_i^j$~--- $j$-й признак у $i$-го объекта. Осталось понять, как выбрать $j$ и $b$.

Для этого будем перебирать все возможные признаки $j$, а для каждого признака перебирать все его возможные значения $x^j$ в выборке, тогда мы можем построить разбиение для каждого из вариантов, осталось выбрать оптимальный. Для этого используют функционал следующего вида:
$$
f(X) = \frac{|L|}{|X|} H(L) + \frac{|R|}{|X|} H(R)
$$
Осталось определить $H$. Как правило, используют одну из следующих двух функций:
\begin{itemize}
    \item Критерий Джини. Имеет формулу $H(X) = \sum\limits_{i = 1}^K p_i \cdot (1 - p_i) = 1 - \sum\limits_{i = 1}^K p_i^2$, где $p_i$~--- выборочные вероятности в $X$ каждого из $K$ классов.
    \item Энтропийный критерий. Имеет формулу $H(X) = \sum\limits_{i = 1}^K p_i \cdot \log p_i$, где $p_i$~--- выборочные вероятности в $X$ каждого из $K$ классов.
\end{itemize}

Тогда находя минимум $f(X)$, выбираем оптимальные $b$ и $j$, разбиваем на два поддерева, рекурсивно повторить.









\subsection{Почему с помощью решающего дерева можно достичь нулевой ошибки на обучающей
выборке без повторяющихся объектов?}

Поскольку объекты не повторяются, каждому набору признаков из представленных в выборке можно сопоставить ровно одно значение целевой переменной, тогда в ветвях дерева можно перебрать все наборы признаков и дать соответствующие им ответы в листьях.














\subsection{Если в лист дерева попали объекты разных классов, то какие предсказания нужно выдавать в этом листе? Почему?}

Если нет задачи получить вектор вероятностей, то просто наиболее часто встречаемый в листе класс.

Если надо еще получить вектор вероятностей для каждого класса, то вернем вектор из частот встречаемости в листе. Подробнее~--- надо расписать лагранжиан.
















\subsection{Какое предсказание нужно выдавать в листе дерева в задаче регрессии если мы
минимизируем MSE? а в случае MAE?}

Дерево в каждом листе предсказывает константу, поэтому будем предсказывать константу оптимальную с точки зрения:

\begin{itemize}
    \item MSE: минимизируем MSE:
    
    $H(R) =\min\limits_c \frac{1}{|R|} \sum\limits_{(x_i, y_i) \in R} (y_i - c)^2$
    
    $c^* = \frac{1}{|R|} \sum \limits_{y_i \in R} y_i$ тобишь среднее
    
    \item MAE: сумма модулей, поэтому предсказываем медиану
\end{itemize}













\subsection{Что такое bagging?}

\Def Bagging = Bootstrap aggregating~--- параллельное обучение элементарных классификаторов на бутстрепных (сгенерированных на основе начального датасета путем равномерного распределения) выборках, выбор итогового результата на основе ответов каждого из них (например, абсолютным большинством).

Сгенерируем бутстрепом $N$ выборок, на каждой обучим модель. Ответом для объекта $x$ будет являться усредненный ответ каждой модели: $\frac{1}{N} \sum \limits_{i=1}^{N} a x_i(x)$













\subsection{Что такое случайный лес? Чем он отличается от бэггинга над решающими деревьями?}

Случайный лес = Random Forest = Bagging + RSM, где 
RSM = Random Subspace Method~--- метод, где для каждого сплита выбирается оптимальный признак для каждого дерева для каждого разбиения внутри него лишь из случайного подмножества признаков (эти подмножества генерируем независимо битовой маской).

То есть в результате деревья в наборе становятся непохожими друг на друга из-за случайности в сплитах. Далее, применяем bagging к набору обученных деревьев.

Таким образом
\begin{enumerate}
    \item Делаем $M$ бутстраппированных датасетов.
    \item На каждой из них обучаем по одному дереву, но на этапе построения \textbf{каждого} разбиения в дереве независимо выбираем подмножество признаков, по которому будет искаться оптимальное разбиение (это нормально, если в итоге лучший признак не попадет туда).
\end{enumerate}














\subsection{Как в градиентном бустинге обучаются базовые алгоритмы?}

Disclaimer: Базовые алгоритмы учатся сообща, помогая друг другу, то есть последовательно, уменьшая при этом ошибку, сделанную предыдущими.

Как? Градиентным спуском в пространстве моделей. Мы двигаемся от модели к модели по антиградиенту функции ошибки. Для этого, очевидно, нам понадобится дифференцируемость этой функции.

Подробнее, пусть имеется модель $\hat{f}_T(X) = \sum\limits_{i = 1}^T \rho_i f_i(X)$, тогда мы можем для данной модели посчитать градиент лосса на объектах выборки. Давайте $(T + 1)$-й алгоритм обучать не под выборку, а как раз под эти градиенты (в некотором смысле шаг градиентного спуска, но спускаемся путем обучения модели под градиент). Тогда найдем оптимальную модель $f_{T + 1}(X)$ и оптимальное $\rho_{T + 1}$. Отсюда получаем, что наш новый алгоритм имеет вид:
$$
\hat{f}_{T + 1}(X) = \sum\limits_{i = 1}^{T + 1} \rho_i f_i(X)
$$






\subsection{Зачем нужен backprop, что такое производная вектора по вектору?}

В нейронной сети Loss считается в самом конце, а обучать мы хотим все слои, значит нам надо научиться прокидывать градиент сквозь все слои вплоть до самого первого. Мы можем это делать, так как знаем, что все слои дифференцируемы, тогда
\begin{equation*}
    \frac{\partial L}{\partial \theta_1} = \frac{\partial L}{\partial f_2(\theta_2)} \cdot \frac{\partial f_2(\theta_2)}{\partial f_1(\theta_1)} \cdot \frac{\partial f_1(\theta_1)}{\partial \theta_1} = \ldots = \frac{\partial L}{\partial f_N(\theta_N)} \prod\limits_{i = 1}^N \frac{\partial f_i(\theta_i)}{\partial f_{i - 1}(\theta_{i - 1})} \cdot \frac{\partial f_1(\theta_1)}{\partial \theta_1}
\end{equation*}

В данной формуле подразумевается, что у нас $N$ слоев, каждый параметризован параметрами $\theta_i$, например, для линейной регрессии это вектор весов.

Производная вектора $y \in \R^n$ по вектору $x \in \R^m$~--- это матрица
$$
\frac{dy}{dx} = \left.\brackets{\frac{dy_i}{dx_j}}\right|_{i, j = 1}^{m, n}
$$









\subsection{Опишите принцип работы свёрточного слоя (CNN).}

Пусть имеется какая-то большая матрица данных $x$ (тензор $W \times H \times C$), наложим на каждый участок этой матрицы фильтр $w$ (тензор $F \times S \times C$)~--- окошко, значения которого умножаются на соответствующие значения участка наших данных. От куска матрицы мы получаем всего одно значение после фильтрации: $w^Tx +b $, в итоге сдвига окошка~--- новую (свёрнутую) матрицу данных (на самом деле тензор $(W - F + 1) \times (H - S + 1) \times 1$), которая отражает меру <<похожести>> данных на фильтр. Внимание: число каналов в фильтре и данных совпадает. Так, мы сгенерировали 1 признаковое представление. Но мы хотим знать больше: поэтому заведем несколько независимых фильтров, работающих по отдельности. В итоге получим новый тензор данных из нескольких слоёв-результатов фильтрации, у которых число измерений совпадает с исходным

На вход приходит трехмерный тензор, характеризующий изображение:  

\begin{center}
    \includegraphics[scale=0.15]{1.png}
\end{center}

К этому изображению применяется операция свертки, для этого используется определенный фильтр – ядро свертки. Требование к нему такое чтобы глубина совпадала с глубиной входного тензора. Пространственные размеры меньше (у исходного тензора был размер 32х32, а у фильтра 5х5).

\begin{center}
    \includegraphics[scale=0.15]{2.png}
\end{center}

Прикладываем этот фильтр ко всем квадратикам 5x5 исходной картинки. Получим:

\begin{center}
    \includegraphics[scale=0.15]{3.png}
\end{center}












\subsection{В чем недостатки полносвязных нейронных сетей какая мотивация к использованию
свёрточных?}

\begin{enumerate}
    \item У полносвязных нейронных сетей очень много обуччаемых параметров, тогда как у сверточного слоя из меньше. Пример, у нас есть слой вида $32 \times 32 \times 3$, хотим получить тензор $32 \times 32 \times 10$. Для сверточного слоя это будет порядка $10 \cdot (5 \cdot 5 \cdot 3 + 1) = 760$ параметров, а для полносвязного $> 3^5 \cdot 10^5$.
    \item Полносвязный слой искренне верит в независимость пикселей, тогда как это неправда, нам важно их взаимное расположение.
\end{enumerate}





\subsection{Опишите принцип работы базового рекуррентного слоя (RNN).}

У нас есть контекст $c$~--- вектор фиксированного размера. Далее нам приходит новое слово, кодируем его как-то, то есть получаем вектор $x$. Конкатенируем $c$ с $x$, но размер такого вектора может банально не влезть в размер контекста, а значит используем линейное преобразование $W[x + c]^T + b$, которое переводит из $\R^{|c| + |x|}$ в $\R^{|c|}$. Далее ставим функцию активации, которая ограничена, например, сигмоида. Она позволяет понять, что запомнить (главное), а что забыть (не главное).









\subsection{Что такое dropout?}

На каждом шаге обучения будем выключать случайные нейроны (домножением выхода на 0 с некоторой вероятностью), чтобы была меньшая зависмость от выхода какого-то конкретного нейрона из прошлого слоя. На тесте включаем все нейронны. Тогда получается что одновременно работает несколько меньших сетей. 

Важно понимать, что нейроны не выбрасываются из сети. Их только отключают на этом шаге. Отключение это домножение результата на 1, то есть на них не происходит обучения, так как до них долетает нулевой градиент.

Вспомним про  нормализацию. 

Так как при обучении на каждом шаге обучалась только половина сети, то получаенная  оценка описывает менее сильные закономерности, но зато она более устойчивая.







\subsection{Как dropout и batch normalization меняют свое поведение при эксплуатации модели (в
режиме inference)?}

Batch Normalization: В режиме train надо обязательно нормировать данные от слоя к слою, потому что каждый раз слой должен ждать на вход одно и то же распределение, иначе ему станет плохо, поэтому есть движущаяся медиана и движущееся среднее, которые обновляются от слоя к слою. В режиме test он нормализует по значениям, которые нашел в train.

Про Dropout: Аналогично нужно “починить” dropout, потому что когда мы после train’a включим режим inference у нас резко включатся совершенно все нейроны, которые изменят распределение. Поэтому надо нормировать выходы, умножая на вероятность dropout’a, чтобы не изменялась дисперсия.








\subsection{Запишите постановку задачи в методе главных компонент.}

Имеется матрица $X$ размера $m \times k$ ранга $N$. Хотим найти такую матрицу $\hat{X}$ того же размера, но ранга $K < N$, которая минимизирует норму Фробениуса между ними (сумма квадратов элементов).

Таким образом, $\hat{X} = \arg\min_{Y \in \text{Mat}_{m \times k}(\R): \ rk Y = K < N} \norm{X - Y}_2$