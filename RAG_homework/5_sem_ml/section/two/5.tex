Наивное предположение: фичи независимы. 

Вспомним теорему Байеса, которая дает нам формулу условной вероятности:

$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$

Ставится задача $K$-классовой классификации (то есть каждый элемент по какому-то признаку может быть одного из $k$ возможных классов). $$x_i \in \R^p, y_i \in \{C_1, \dots, C_k\}$$

Давайте прикинем условную вероятность метки класса при условии того, что объект описывается какими-то фичами.

$$P(y_i = C_k|x_i) = \frac{P(x_i|y_i = C_k)P(y_i = C_k)}{P(x_i)}$$

То есть мы оцениваем, какова вероятность того, что метка класса это $C_k$ при условии, что мы наблюдаем $x_i$.

Уже имея данную формулу мы можем оценивать вероятность лейблов. $P(y_i = C_k)$ оценим как частоту (встречаемости) каждого класса в нашей выборке (просто руками посчитаем, насколько вероятно попадание в тот или иной класс). Но нам надо оценить $P(x_i|y_i = C_k)$. Это сделать непонятно, как, потому что $x_i$ -- это вектор. Здесь и понадобится наше \textit{наивное} предположение - все признаки независимы и тогда хорошо факторизуются (мы считаем, что признаков у нас $p$).

$$P(x_i|y_i = C_k) = \prod\limits_{n = 1}^p P(x_i^n|y_i = C_k)$$

В реальной жизни признаки часто зависимы, причем они бывают зависимы довольно сложным образом, так что просто так взять и подчистить данные не удастся. Но есть хорошая новость -- признаки бывают частично независимы (например, среди них бывают независимые подмножества), так что какой-то смысл в этом есть. Теперь вектор $x_i$ представляется как вектор $p$ независимых случайных величин. Мы хотим найти метку класса, которая наиболее вероятна

$$C^* = \underset{k}{argmax} ~ P(y_i = C_k|x_i) = \underset{k}{argmax} ~ \frac{P(x_i|y_i = C_k)P(y_i = C_k)}{P(x_i)}$$ Это и будет нашим предсказанием.\\

Теперь заметим, что знаменатель не зависит от $k$, поэтому на нахождение максимума он не влияет. То есть если его убрать, у нас, конечно, выражение перестанет быть вероятностью, но метка класса, при котором достигается этот самый максимум, не изменится. Поэтому на него можно забить, и даже сэкономить на этом какое-то время (хотя и не очень большое).

Правда, на самом деле, мы в любом случае его и так практически посчитаем по пути, так как $P(x_i) = \sum_kP(x_i|y_i=C_k)$.\\

Для того, чтобы посчитать $P(x_i^l|y_i = C_k)$, требуется ввести какое-то распределение на наши признаки. Строго понять это вряд ли получится, так что или нам это было известно, или мы должны просто предположить, из какого распределения они пришли. Тут есть 2 пути: 1 - если они тоже все категориальные, можем просто посчитать частоту каждого из них, 2 - если они какие-то числовые, то тут чуть сложнее, нам придется ввести какое-то априорное распределение на $x$. Самый простой случай, когда $x$ пришел из нормального распределения. В этом случае мы считаем дисперсию и среднее по выборке, предполагая что распределение нормальное, и тогда можем оценить вероятность.

Такой классификатор будет хорошо работать на классификации текстов (например, определять, грубый ли комментарий в соц.сети).