Функция правдоподобия это ни что иное как условная вероятность выборки, при условии что
параметрические семейство, которое описывает эту выборку обладает ровно таким параметром.
$$L(\theta | X, Y ) = P(X, Y |\theta)$$
От правдоподобия мы хотим только одну вещь - максимизировать. Хотим наиболее правдоподобные параметры
при условии нашей выборки.
$$L(\theta | X, Y ) \rightarrow \underset{\theta}{\max}$$
Так как матрица признаков имеет незаивисимые объекты i.i.d, мы можем расписать условную вероятность
как произведение условной вероятности по каждому объекту.
$$L(\theta|X, Y ) = P(X, Y |\theta) = \prod\limits_i P(x_i, y_i |\theta)$$
Argmax функции совпадает с argmax любого монотонного преобразования над функцией. Тут можно использовать логарифм, который из произведения делает сумму, логарифм - монотонное преобразование, поэтому
мы можем заменить произведение на сумму логарифмов условной вероятности и его уже максимизировать:
$$log L(\theta|X, Y ) = P(X, Y |\theta) = \sum\limits_i log P(x
_i, y_i |\theta)  \rightarrow \underset{\theta}{\max}$$
\\
\begin{note}
ФУНКЦИЯ ПРАВДОПОДОБИЯ - ЭТО НЕ РАСПРЕДЕЛЕНИЕ НАД ПАРАМЕТРАМИ $\theta$

\end{note}

\begin{note}
\
\\

Большинство моделей работает по принципу максимального правдоподобия, линейная регрессия и классфикация работают именно по нему.

В задаче линейной регресси мы ходим минимизировать MSE для получения оптимального вектора весов. Задача минимизации MSE эквивалентна методу максимального правдоподобия при определенных условиях по теореме Гаусса-Маркова.
\\
В задаче классификации мы напрямую используем MLE для выбора параметров
\end{note}

