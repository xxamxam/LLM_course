\textbf{Постановка задачи}

$X \in \R^{n \times p}, Y \in C^n$, где $C$ -- это множество меток классов (конечное). Если метки не упорядочены, то это называется \textit{задачей классификации}.

Пока что будет заниматься бинарной классификацией и для удобства будем считать, что \(C = \{-1, 1\}\). Обозначим за $c(X)$ функцию, которая возвращает $\widehat{Y}$, то есть предсказание классов нашей модели.

Допустим у нас есть матрица признаков и вектор весов, как из этого всего, что уже было в линейной регрессии,
составить линейную классификацию?
Надо делить на классы предсказанный ответ, обрубая по какой-то границе, если у нас два класса.

\[
c(x) = 
\begin{cases}
1, f(x) \geqslant 0 \\
-1, f(x) < 0
\end{cases}
\]

Пусть вектор $w$ является нормалью к гиперплоскости в $p$-мерном пространстве фичей (гиперплоскость задаётся уравнением $x^T w = 0$), с одной стороны гиперплоскости будут одного класса объекты, с другой --- другого класса объекта. Тогда $c(x)$ можно переписать в следующем виде:

\[
c(x) = \sgn(f(x)) = \sgn(x^T w)
\]

Введём понятие margin (отступ):

\[
M_i = y_i f(x_i) = y_i x_i^T w
\]

Это некоторый аналог ориентированного расстояния, причём если $M_i \leqslant 0$, значит объект классифицирован неправильно. Тогда логично предложить следующую функцию потерь:

\[
\text{Loss} = \sum_{\text{by objects}} [M_i \leqslant 0]
\]

Но у такой loss function будут проблемы, так как это не гладкая функция. Это главная причина почему мы
не будем использовать такой loss, по крайней мере в наших текущих задачах. Есть ещё один менее очевидный
минус: такая метрика ничего нам не говорит об уверенности классификатора. Представьте объекты как точки
в пространстве признаков, есть гиперплоскость классификатора, и было бы разумно полагать, что объекты
на границе, где перемешиваются два класса, будут с меньшей уверенностью разделены классификатором, чем
лежащие далеко "в толще"класса, которые с большей точностью принадлежат своему классу.

Что можно сделать с имеющейся функцией потерь --- приблизить её к гладкой функции. Предлагается использовать логистическую регрессию.

\[
p_+ = P(y = 1|x) \in [0, 1]
\]

Регрессионная модель живет в пространстве $\R$, а вероятность живёт исключительно в промежутке [0,1]. Как с этим быть?
Применить функцию, которая переводит R в промежуток [0,1] и таким образом моделлирует вероятность.
Используем некий трюк и составим величину, которая уже будет куда ближе к R:
$\frac{p_+}{1 - p_+} \in [0, +\infty)$

Осталось отразить промежуток $[0, +\infty]$ в $\R$, с чем хорошо справляется функция логарифма:
$\log \frac{p_+}{1 - p_+} \in \R$
Таким образом мы объединили мир классификации и мир регрессии. Теперь запишем предсказание через
вектор признаков и вектор весов, а также выразим вероятность объекта иметь класс 1:
\[
\frac{p_+}{1 - p_+} = \exp(x^T w)
\]
\[
p_+ = \frac{1}{1 + \exp(-x^T w)} = \sigma(x^T w)
\]
Получилось, что вероятность объекта иметь 1 класс равна сигмоиде от привычного выражения для линейной
регрессии.