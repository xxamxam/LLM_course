Напомним, что аналитическое решение для метода наименьших квадратов выглядит следующим образом:

\[
\widehat{w} = (X^T X)^{-1} X^T Y
\]

Видно, что нам приходится искать матрицу, обратную к $X^T X$. Однако, что делать, если эта матрица вырождена (или близка к вырожденной)? 

Сначала подумаем, что означает вырожденность матрицы $X^T X$. Это означает, что в этой матрице (а значит, и в матрице $X$) есть линейно зависимые столбцы (признаки). В таком случае столбец весов не может быть определён однозначно, так как будет фиксирована только сумма весов этих параметров, а сами параметры могут определяться континуальным количеством способов.

Если же матрица $X^T X$ близка к вырожденной, то решение получается \textit{нестабильным} (то есть при добавлении небольшого шума в данные вектор весов меняется сильно).

Чтобы решить эту проблему, нам хочется ограничить свободу выбора этих коэффициентов, наложить дополнительное условие, чтобы решение стало единственным. Давайте потребуем, чтобы норма вектора весов была наименьшей из возможных. Тогда задача оптимизации запишется следующим образом:

\[
\widehat{w} = \underset{w}{\argmin}
\left( \|Y - Xw\|_2^2 + \lambda \|w\|_2^2 \right)
\]

Коэффициент $\lambda$ -- это гиперпараметр, который отвечает за то, насколько важно для нас, чтобы норма вектора весов была маленькая.

\textbf{Важное замечание.}
В слагаемом \(\lambda \|w\|_2^2\) мы записываем $w$ без свободного члена ($w_0$), так как иначе наша модель будет стараться уменьшать в том числе и $w_0$, то есть пытаться провести гиперплоскость через ноль. В общем случае это не следует не из каких предположений, поэтому накладывать ограничение на $w_0$ нельзя.

Аналитическое решение такой задачи будет выглядеть следующим образом:

\[
\widehat{w} = \left( X^T X + \lambda I \right)^{-1} X^T Y
\]

Видно, что теперь обратная матрица существует всегда (так как  к $X^T X$ мы добавляем единичную матрицу).

Подход, при котором мы накладываем дополнительные ограничение, называется \textit{регуляризацией}. При этом не обязательно накладывается ограничение на норму вектора весов (см. другие примеры в билете \ref{27}).

Можно применять не только $L2$ регуляризацию, но и $L1$.

\textbf{L2}
\begin{itemize}
    \item имеет аналитическое решение
    \item дифференцируема
\end{itemize}

\textbf{L1}
\begin{itemize}
    \item не дифференцируема
    \item "отбирает" признаки
\end{itemize}